import sqlite3
from gensim.models import Word2Vec
from faiss import IndexFlatL2
import openai

# Replace with your OpenAI API key
openai.api_key = "YOUR_OPENAI_API_KEY"

# Connect to SQLite database (modify filename if needed)
conn = sqlite3.connect("conversation_history.db")
cursor = conn.cursor()

# Create table (modify if needed)
cursor.execute("""CREATE TABLE IF NOT EXISTS conversation_history (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  prompt TEXT NOT NULL,
  response TEXT NOT NULL,
  embedding BLOB
);""")
conn.commit()


def get_response(prompt):
  """
  Sends prompt to Azure OpenAI and retrieves response.

  Args:
      prompt: The prompt to send to the LLM.

  Returns:
      The response generated by the LLM.
  """
  response = openai.Completion.create(
      engine="text-davinci-003",
      prompt=prompt,
      max_tokens=1024,
      n=1,
      stop=None,
      temperature=0.7,
  )
  return response.choices[0].text.strip()


def store_conversation(prompt, response):
  """
  Stores conversation history (prompt and response) in SQLite with embeddings.

  Args:
      prompt: The user prompt for the conversation.
      response: The LLM generated response.
  """
  # Combine prompt and response for embedding
  conversation = f"{prompt}\n{response}"
  embedding = generate_embedding(conversation)  # Generate embedding for the conversation

  cursor.execute("INSERT INTO conversation_history (prompt, response, embedding) VALUES (?, ?, ?)", (prompt, response, embedding))
  conn.commit()


def generate_embedding(text):
  """
  Generates text embedding using gensim Word2Vec.

  Args:
      text: The text to generate embedding for.

  Returns:
      A list representing the text embedding.
  """
  model = Word2Vec([text], min_count=1, size=100)  # Adjust parameters as needed
  return model.wv[text].tolist()


def retrieve_similar_conversations(new_prompt, k=3):
  """
  Retrieves similar conversations from SQLite based on a new prompt using faiss.

  Args:
      new_prompt: The new prompt/query for similarity search.
      k: Number of most similar conversations to retrieve (default: 3).

  Returns:
      A list of tuples containing conversation IDs, prompts, responses, and similarity scores.
  """
  embeddings = []
  prompts = []
  responses = []

  # Retrieve all conversation embeddings and data from database
  cursor.execute("SELECT prompt, response, embedding FROM conversation_history")
  for row in cursor.fetchall():
    prompts.append(row[0])
    responses.append(row[1])
    embeddings.append(row[2])

  # Create faiss index for efficient similarity search
  index = IndexFlatL2(len(embeddings[0]))  # Assuming all embeddings have same dimensionality
  index.add(embeddings)

  # Generate embedding for the new prompt
  new_embedding = generate_embedding(new_prompt)

  # Perform similarity search using faiss
  distances, indices = index.search(np.array([new_embedding]), k)

  # Retrieve similar conversations based on indices
  similar_conversations = []
  for i in range(k):
    similar_conversations.append((prompts[indices[0][i]], responses[indices[0][i]], distances[0][i]))

  return similar_conversations


def main():
  while True:
    user_input = input("You: ")
    prompt = user_input

    # Get response from Azure OpenAI
    response = get_response(prompt)
    print(f"LLM: {response}")

    # Store conversation with context (prompt and response)
    store_conversation(prompt, response)

    # Retrieve similar conversations (optional)
    similar_conversations = retrieve_similar_conversations(user_input)
    if similar_conversations:
      print("Similar Conversations:")
      for conversation in similar_conversations:
        print(f"- Prompt: {conversation[0]} (Similarity Score: {conversation[2]:.2f})")
        print(f"- Response: {conversation[1]}
